{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['test.csv', 'train.csv', 'sample_submission.csv']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\nfrom collections import Counter","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/train.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(10)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   Id                                             review  sentiment\n0   0  I have done a lot of international travel, bot...          1\n1   1  One of the most frightening game experiences e...          1\n2   2  I was amazingly impressed by this movie. It co...          1\n3   3  This film is stale, and misses the mark. It is...          0\n4   4  At last!! Sandra Bullock is indeed a beautiful...          1\n5   5  This film is a very good movie.The way how the...          1\n6   6  \"Carriers\" follows the exploits of two guys an...          0\n7   7  I admit I have a weakness for alternate histor...          1\n8   8  I'm a fan of the 1950's original and about 20 ...          0\n9   9  A below average looking video game is turned i...          0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I have done a lot of international travel, bot...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>One of the most frightening game experiences e...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>I was amazingly impressed by this movie. It co...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>This film is stale, and misses the mark. It is...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>At last!! Sandra Bullock is indeed a beautiful...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>This film is a very good movie.The way how the...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>\"Carriers\" follows the exploits of two guys an...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>I admit I have a weakness for alternate histor...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>I'm a fan of the 1950's original and about 20 ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>A below average looking video game is turned i...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = []\nlabels =[]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for r in data['review'].str.lower():\n    reviews.append(r)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for l in data['sentiment']:\n    labels.append(l)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels[:10]","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"[1, 1, 1, 0, 1, 1, 0, 1, 0, 0]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(reviews)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for review in reviews:\n    print (review)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get rid of punctuation\nall_text = ''.join([c for c in reviews if c not in punctuation])\n\n# split by new lines and spaces\nreviews_split = all_text.split('\\n')\nall_text = ' '.join(reviews_split)\n\n# create a list of words\nwords = all_text.split()","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[:30]","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"['i',\n 'have',\n 'done',\n 'a',\n 'lot',\n 'of',\n 'international',\n 'travel,',\n 'both',\n 'on',\n 'business',\n 'and',\n 'as',\n 'a',\n 'tourist.',\n 'for',\n 'both',\n 'types',\n 'i',\n 'assure',\n 'you',\n 'the',\n 'best',\n 'advice',\n 'is',\n 'also',\n 'the',\n 'oldest:',\n 'always',\n 'drink']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Build a dictionary that maps words to integers\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\nvocab_to_int","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"{'the': 1,\n 'a': 2,\n 'and': 3,\n 'of': 4,\n 'to': 5,\n 'is': 6,\n 'in': 7,\n 'i': 8,\n 'this': 9,\n 'that': 10,\n 'it': 11,\n '/><br': 12,\n 'was': 13,\n 'as': 14,\n 'with': 15,\n 'for': 16,\n 'but': 17,\n 'on': 18,\n 'movie': 19,\n 'his': 20,\n 'are': 21,\n 'not': 22,\n 'you': 23,\n 'film': 24,\n 'have': 25,\n 'he': 26,\n 'be': 27,\n 'at': 28,\n 'one': 29,\n 'by': 30,\n 'an': 31,\n 'they': 32,\n 'from': 33,\n 'who': 34,\n 'all': 35,\n 'like': 36,\n 'so': 37,\n 'just': 38,\n 'or': 39,\n 'has': 40,\n 'about': 41,\n 'her': 42,\n \"it's\": 43,\n 'some': 44,\n 'if': 45,\n 'out': 46,\n 'what': 47,\n 'very': 48,\n 'when': 49,\n 'more': 50,\n 'there': 51,\n 'would': 52,\n 'even': 53,\n 'good': 54,\n 'she': 55,\n 'my': 56,\n 'only': 57,\n 'their': 58,\n 'no': 59,\n 'really': 60,\n 'had': 61,\n 'up': 62,\n 'can': 63,\n 'which': 64,\n 'see': 65,\n 'were': 66,\n 'than': 67,\n 'we': 68,\n '-': 69,\n 'been': 70,\n 'get': 71,\n 'into': 72,\n 'will': 73,\n 'because': 74,\n 'much': 75,\n 'most': 76,\n 'story': 77,\n 'how': 78,\n 'other': 79,\n 'also': 80,\n 'its': 81,\n 'do': 82,\n \"don't\": 83,\n 'time': 84,\n 'me': 85,\n 'great': 86,\n 'first': 87,\n 'people': 88,\n 'make': 89,\n '/>the': 90,\n 'could': 91,\n 'any': 92,\n 'then': 93,\n 'bad': 94,\n 'made': 95,\n 'after': 96,\n 'think': 97,\n 'many': 98,\n 'never': 99,\n 'him': 100,\n 'being': 101,\n 'two': 102,\n 'too': 103,\n 'where': 104,\n 'little': 105,\n '<br': 106,\n 'well': 107,\n 'watch': 108,\n 'way': 109,\n 'your': 110,\n 'did': 111,\n 'them': 112,\n 'know': 113,\n 'best': 114,\n 'love': 115,\n 'characters': 116,\n 'does': 117,\n 'seen': 118,\n 'character': 119,\n 'movies': 120,\n 'these': 121,\n 'ever': 122,\n 'it.': 123,\n 'still': 124,\n 'movie.': 125,\n 'over': 126,\n 'show': 127,\n 'films': 128,\n 'such': 129,\n 'acting': 130,\n 'should': 131,\n 'plot': 132,\n 'those': 133,\n 'off': 134,\n 'better': 135,\n 'while': 136,\n 'something': 137,\n 'say': 138,\n 'go': 139,\n 'through': 140,\n \"doesn't\": 141,\n 'why': 142,\n \"didn't\": 143,\n 'scene': 144,\n 'makes': 145,\n 'film,': 146,\n 'movie,': 147,\n 'real': 148,\n 'film.': 149,\n 'watching': 150,\n 'find': 151,\n 'scenes': 152,\n 'actually': 153,\n 'back': 154,\n 'man': 155,\n \"i'm\": 156,\n 'every': 157,\n '/>i': 158,\n 'going': 159,\n 'few': 160,\n 'life': 161,\n 'same': 162,\n 'lot': 163,\n 'look': 164,\n 'nothing': 165,\n 'another': 166,\n 'new': 167,\n 'thing': 168,\n 'quite': 169,\n 'end': 170,\n 'want': 171,\n 'old': 172,\n 'pretty': 173,\n 'seems': 174,\n 'got': 175,\n '&': 176,\n \"can't\": 177,\n 'give': 178,\n 'take': 179,\n 'before': 180,\n 'actors': 181,\n 'part': 182,\n 'years': 183,\n 'young': 184,\n 'may': 185,\n 'us': 186,\n \"that's\": 187,\n 'big': 188,\n 'both': 189,\n 'between': 190,\n 'without': 191,\n 'things': 192,\n 'thought': 193,\n 'gets': 194,\n 'it,': 195,\n 'around': 196,\n 'though': 197,\n 'almost': 198,\n \"isn't\": 199,\n 'director': 200,\n 'must': 201,\n 'saw': 202,\n 'always': 203,\n 'here': 204,\n 'now': 205,\n 'whole': 206,\n \"i've\": 207,\n 'come': 208,\n 'horror': 209,\n 'might': 210,\n 'own': 211,\n 'down': 212,\n 'work': 213,\n \"he's\": 214,\n 'am': 215,\n 'cast': 216,\n 'bit': 217,\n \"there's\": 218,\n '\"the': 219,\n 'enough': 220,\n 'least': 221,\n 'probably': 222,\n 'last': 223,\n 'long': 224,\n 'feel': 225,\n 'since': 226,\n 'funny': 227,\n 'far': 228,\n 'kind': 229,\n 'each': 230,\n 'rather': 231,\n 'fact': 232,\n 'original': 233,\n 'found': 234,\n 'our': 235,\n 'guy': 236,\n 'world': 237,\n 'worst': 238,\n 'interesting': 239,\n 'making': 240,\n 'done': 241,\n 'trying': 242,\n 'comes': 243,\n 'right': 244,\n 'anything': 245,\n 'action': 246,\n '/>this': 247,\n 'believe': 248,\n 'played': 249,\n 'put': 250,\n 'main': 251,\n 'having': 252,\n 'point': 253,\n 'anyone': 254,\n 'however,': 255,\n 'music': 256,\n 'worth': 257,\n 'role': 258,\n 'goes': 259,\n 'hard': 260,\n 'looking': 261,\n 'yet': 262,\n 'tv': 263,\n 'especially': 264,\n \"wasn't\": 265,\n 'looks': 266,\n 'watched': 267,\n 'performance': 268,\n 'plays': 269,\n 'family': 270,\n 'minutes': 271,\n 'seem': 272,\n 'series': 273,\n 'someone': 274,\n 'during': 275,\n 'takes': 276,\n 'shows': 277,\n 'different': 278,\n 'sure': 279,\n 'script': 280,\n 'comedy': 281,\n 'three': 282,\n 'away': 283,\n 'maybe': 284,\n 'woman': 285,\n 'times': 286,\n 'everything': 287,\n 'set': 288,\n 'american': 289,\n 'left': 290,\n 'girl': 291,\n 'although': 292,\n 'seeing': 293,\n 'play': 294,\n \"you're\": 295,\n 'fun': 296,\n 'simply': 297,\n 'once': 298,\n 'used': 299,\n 'completely': 300,\n 'everyone': 301,\n 'special': 302,\n 'john': 303,\n 'again': 304,\n 'reason': 305,\n 'time.': 306,\n 'read': 307,\n 'need': 308,\n 'true': 309,\n 'idea': 310,\n 'black': 311,\n 'sense': 312,\n 'use': 313,\n '--': 314,\n 'truly': 315,\n 'dvd': 316,\n 'given': 317,\n 'high': 318,\n 'until': 319,\n 'came': 320,\n 'help': 321,\n 'well,': 322,\n 'beautiful': 323,\n 'recommend': 324,\n 'nice': 325,\n 'place': 326,\n 'rest': 327,\n 'job': 328,\n 'ending': 329,\n 'version': 330,\n 'less': 331,\n 'getting': 332,\n 'couple': 333,\n 'try': 334,\n 'full': 335,\n 'keep': 336,\n 'half': 337,\n 'actor': 338,\n 'effects': 339,\n 'along': 340,\n 'tell': 341,\n 'shot': 342,\n 'gives': 343,\n 'let': 344,\n 'second': 345,\n 'poor': 346,\n 'instead': 347,\n 'enjoy': 348,\n 'said': 349,\n 'money': 350,\n 'excellent': 351,\n '(and': 352,\n 'himself': 353,\n 'playing': 354,\n 'audience': 355,\n 'understand': 356,\n \"couldn't\": 357,\n 'early': 358,\n 'definitely': 359,\n 'absolutely': 360,\n 'day': 361,\n 'next': 362,\n 'remember': 363,\n 'fan': 364,\n 'went': 365,\n 'start': 366,\n 'supposed': 367,\n 'certainly': 368,\n 'book': 369,\n 'together': 370,\n 'war': 371,\n 'short': 372,\n 'liked': 373,\n 'screen': 374,\n 'doing': 375,\n 'entire': 376,\n 'become': 377,\n 'later': 378,\n 'often': 379,\n 'several': 380,\n 'all,': 381,\n 'felt': 382,\n 'kids': 383,\n 'small': 384,\n 'sort': 385,\n 'human': 386,\n 'totally': 387,\n '2': 388,\n 'men': 389,\n 'perhaps': 390,\n 'piece': 391,\n 'loved': 392,\n 'is,': 393,\n 'seemed': 394,\n 'hollywood': 395,\n 'against': 396,\n 'wife': 397,\n 'night': 398,\n '(the': 399,\n 'time,': 400,\n 'wonderful': 401,\n 'production': 402,\n 'year': 403,\n \"she's\": 404,\n 'waste': 405,\n 'else': 406,\n 'star': 407,\n 'becomes': 408,\n 'wanted': 409,\n 'camera': 410,\n 'course': 411,\n 'able': 412,\n 'top': 413,\n 'hope': 414,\n 'final': 415,\n 'women': 416,\n 'classic': 417,\n 'that,': 418,\n 'home': 419,\n 'line': 420,\n 'friends': 421,\n 'based': 422,\n 'video': 423,\n '.': 424,\n 'death': 425,\n \"you'll\": 426,\n 'gave': 427,\n '\\x96': 428,\n 'house': 429,\n 'mind': 430,\n 'live': 431,\n 'called': 432,\n 'performances': 433,\n 'school': 434,\n 'lost': 435,\n 'person': 436,\n 'father': 437,\n '10': 438,\n 'wants': 439,\n 'stupid': 440,\n \"i'd\": 441,\n 'lead': 442,\n 'name': 443,\n 'tries': 444,\n 'turn': 445,\n 'sound': 446,\n 'perfect': 447,\n \"they're\": 448,\n 'already': 449,\n 'sex': 450,\n \"won't\": 451,\n 'them.': 452,\n 'either': 453,\n 'this,': 454,\n 'under': 455,\n 'story,': 456,\n 'low': 457,\n 'finally': 458,\n 'dead': 459,\n 'enjoyed': 460,\n 'care': 461,\n 'despite': 462,\n 'written': 463,\n 'mean': 464,\n 'guess': 465,\n 'head': 466,\n 'starts': 467,\n 'took': 468,\n 'favorite': 469,\n 'budget': 470,\n 'face': 471,\n 'moments': 472,\n 'problem': 473,\n 'turns': 474,\n 'him.': 475,\n 'and,': 476,\n 'me,': 477,\n 'episode': 478,\n 'kill': 479,\n 'lines': 480,\n 'behind': 481,\n 'terrible': 482,\n 'extremely': 483,\n 'white': 484,\n 'dialogue': 485,\n 'title': 486,\n 'friend': 487,\n 'boring': 488,\n 'mother': 489,\n 'lack': 490,\n 'itself': 491,\n 'looked': 492,\n 'cannot': 493,\n 'beginning': 494,\n 'others': 495,\n 'fine': 496,\n 'guys': 497,\n \"wouldn't\": 498,\n 'michael': 499,\n 'dark': 500,\n 'highly': 501,\n 'well.': 502,\n 'quality': 503,\n 'group': 504,\n 'decent': 505,\n '/>in': 506,\n 'laugh': 507,\n 'save': 508,\n 'heard': 509,\n 'mr.': 510,\n 'fans': 511,\n 'wrong': 512,\n 'story.': 513,\n 'stars': 514,\n 'boy': 515,\n 'obviously': 516,\n \"film's\": 517,\n 'good,': 518,\n '/>it': 519,\n 'good.': 520,\n 'taken': 521,\n 'evil': 522,\n 'expect': 523,\n 'particularly': 524,\n 'lives': 525,\n 'complete': 526,\n 'one.': 527,\n 'case': 528,\n 'entertaining': 529,\n 'late': 530,\n 'sometimes': 531,\n 'style': 532,\n 'throughout': 533,\n 'feeling': 534,\n 'attempt': 535,\n 'picture': 536,\n 'works': 537,\n 'me.': 538,\n 'exactly': 539,\n 'run': 540,\n '/>if': 541,\n 'whose': 542,\n 'movies,': 543,\n 'this.': 544,\n 'directed': 545,\n 'wonder': 546,\n 'films,': 547,\n 'told': 548,\n 'across': 549,\n 'type': 550,\n 'leave': 551,\n '3': 552,\n 'fight': 553,\n 'living': 554,\n 'huge': 555,\n 'writing': 556,\n 'course,': 557,\n 'usually': 558,\n 'says': 559,\n 'all.': 560,\n 'opening': 561,\n 'worse': 562,\n 'coming': 563,\n 'shown': 564,\n 'thinking': 565,\n 'killer': 566,\n 'close': 567,\n 'major': 568,\n 'car': 569,\n 'except': 570,\n 'viewer': 571,\n 'wish': 572,\n 'known': 573,\n 'killed': 574,\n 'awful': 575,\n 'game': 576,\n 'soon': 577,\n 'taking': 578,\n 'life.': 579,\n 'stop': 580,\n 'finds': 581,\n 'parts': 582,\n 'amazing': 583,\n 'acting,': 584,\n 'happens': 585,\n 'running': 586,\n 'strong': 587,\n 'past': 588,\n 'police': 589,\n 'number': 590,\n 'turned': 591,\n 'children': 592,\n 'somewhat': 593,\n 'hour': 594,\n 'direction': 595,\n 'side': 596,\n 'tells': 597,\n ',': 598,\n 'act': 599,\n 'knew': 600,\n 'local': 601,\n 'myself': 602,\n 'female': 603,\n 'it.<br': 604,\n 'call': 605,\n 'single': 606,\n 'obvious': 607,\n 'matter': 608,\n 'fact,': 609,\n 'horrible': 610,\n 'serious': 611,\n 'humor': 612,\n 'girls': 613,\n 'robert': 614,\n 'brilliant': 615,\n 'none': 616,\n 'including': 617,\n 'whether': 618,\n 'started': 619,\n 'characters,': 620,\n 'due': 621,\n 'david': 622,\n 'british': 623,\n 'james': 624,\n 'movies.': 625,\n 'here,': 626,\n 'again,': 627,\n 'hit': 628,\n 'way,': 629,\n 'involved': 630,\n 'cinema': 631,\n 'son': 632,\n 'mostly': 633,\n 'town': 634,\n \"aren't\": 635,\n 'chance': 636,\n 'giving': 637,\n 'beyond': 638,\n 'actress': 639,\n 'bring': 640,\n 'drama': 641,\n 'important': 642,\n 'order': 643,\n 'art': 644,\n 'knows': 645,\n '/>there': 646,\n 'voice': 647,\n 'cut': 648,\n 'talking': 649,\n 'saying': 650,\n 'end,': 651,\n 'relationship': 652,\n 'clearly': 653,\n 'upon': 654,\n 'one,': 655,\n 'supporting': 656,\n 'themselves': 657,\n 'stuff': 658,\n 'bad,': 659,\n 'eyes': 660,\n 'falls': 661,\n 'ends': 662,\n 'released': 663,\n 'bad.': 664,\n \"haven't\": 665,\n \"i'll\": 666,\n 'however': 667,\n 'simple': 668,\n 'but,': 669,\n 'kid': 670,\n 'using': 671,\n 'english': 672,\n 'history': 673,\n 'four': 674,\n 'french': 675,\n 'similar': 676,\n 'heart': 677,\n 'modern': 678,\n 'near': 679,\n 'lots': 680,\n 'named': 681,\n 'showing': 682,\n 'him,': 683,\n 'moment': 684,\n 'musical': 685,\n 'certain': 686,\n 'appears': 687,\n 'actual': 688,\n 'example': 689,\n 'song': 690,\n 'among': 691,\n '/>but': 692,\n 'stories': 693,\n 'movie.<br': 694,\n 'add': 695,\n 'change': 696,\n 'easily': 697,\n 'that.': 698,\n 'child': 699,\n 'kept': 700,\n 'needs': 701,\n 'score': 702,\n 'seen.': 703,\n 'out.': 704,\n 'days': 705,\n 'mention': 706,\n 'greatest': 707,\n 'tried': 708,\n 'here.': 709,\n 'apparently': 710,\n 'yes,': 711,\n 'blood': 712,\n 'strange': 713,\n 'city': 714,\n 'feels': 715,\n 'interest': 716,\n 'happened': 717,\n '/>a': 718,\n 'nearly': 719,\n 'within': 720,\n 'fall': 721,\n 'hours': 722,\n 'plot,': 723,\n 'life,': 724,\n 'basically': 725,\n 'murder': 726,\n 'overall': 727,\n 'daughter': 728,\n 'bunch': 729,\n 'comic': 730,\n '(i': 731,\n 'five': 732,\n '/>and': 733,\n 'usual': 734,\n 'slow': 735,\n 'romantic': 736,\n 'brought': 737,\n 'happy': 738,\n 'film.<br': 739,\n 'typical': 740,\n 'yourself': 741,\n 'buy': 742,\n 'shots': 743,\n 'miss': 744,\n 'end.': 745,\n \"you've\": 746,\n 'hate': 747,\n 'middle': 748,\n 'working': 749,\n 'jack': 750,\n 'sad': 751,\n '(which': 752,\n 'cheap': 753,\n 'talk': 754,\n 'surprised': 755,\n 'stay': 756,\n '/>as': 757,\n 'funny,': 758,\n 'hell': 759,\n 'her.': 760,\n 'ten': 761,\n 'is.': 762,\n 'word': 763,\n 'happen': 764,\n '(as': 765,\n 'so,': 766,\n 'view': 767,\n 'decided': 768,\n 'above': 769,\n 'sit': 770,\n 'george': 771,\n 'events': 772,\n 'films.': 773,\n 'famous': 774,\n 'power': 775,\n 'flick': 776,\n 'jokes': 777,\n 'cool': 778,\n \"what's\": 779,\n 'brother': 780,\n 'funny.': 781,\n 'deal': 782,\n 'way.': 783,\n 'silly': 784,\n 'sets': 785,\n 'talent': 786,\n 'annoying': 787,\n 'documentary': 788,\n 'hear': 789,\n 'became': 790,\n 'easy': 791,\n 'age': 792,\n 'on.': 793,\n 'attention': 794,\n 'body': 795,\n 'also,': 796,\n 'gore': 797,\n 'peter': 798,\n 'filmed': 799,\n 'experience': 800,\n 'begins': 801,\n 'again.': 802,\n 'please': 803,\n 'television': 804,\n 'keeps': 805,\n 'emotional': 806,\n 'violence': 807,\n 'killing': 808,\n 'straight': 809,\n 'poorly': 810,\n 'moving': 811,\n 'leaves': 812,\n 'cinematography': 813,\n 'learn': 814,\n 'alone': 815,\n 'reality': 816,\n 'difficult': 817,\n \"who's\": 818,\n 'them,': 819,\n 'genre': 820,\n 'towards': 821,\n 'light': 822,\n 'character,': 823,\n 'husband': 824,\n '5': 825,\n 'gone': 826,\n 'eventually': 827,\n 'richard': 828,\n 'hero': 829,\n 'sequence': 830,\n 'means': 831,\n 'meets': 832,\n 'clear': 833,\n \"/>it's\": 834,\n 'ridiculous': 835,\n 'nor': 836,\n 'brings': 837,\n 'imagine': 838,\n '1': 839,\n 'characters.': 840,\n '4': 841,\n 'out,': 842,\n 'oh': 843,\n 'possible': 844,\n '(or': 845,\n 'god': 846,\n 'episodes': 847,\n 'subject': 848,\n 'doubt': 849,\n 'incredibly': 850,\n 'problems': 851,\n 'rent': 852,\n 'realize': 853,\n 'move': 854,\n 'hand': 855,\n 'check': 856,\n 'previous': 857,\n 'write': 858,\n 'various': 859,\n 'possibly': 860,\n 'de': 861,\n 'ones': 862,\n 'forget': 863,\n 'sexual': 864,\n 'somehow': 865,\n 'hilarious': 866,\n 'theme': 867,\n 'figure': 868,\n 'unfortunately': 869,\n 'roles': 870,\n 'songs': 871,\n 'comments': 872,\n 'reading': 873,\n 'dialog': 874,\n 'country': 875,\n 'too.': 876,\n 'fairly': 877,\n 'whom': 878,\n 'personal': 879,\n 'elements': 880,\n 'on,': 881,\n 'room': 882,\n 'paul': 883,\n 'review': 884,\n 'stand': 885,\n 'words': 886,\n 'plenty': 887,\n 'leads': 888,\n 'forced': 889,\n 'needed': 890,\n 'level': 891,\n 'message': 892,\n 'avoid': 893,\n 'feature': 894,\n 'career': 895,\n 'open': 896,\n 'total': 897,\n 'writer': 898,\n 'leading': 899,\n 'manages': 900,\n 'male': 901,\n 'enjoyable': 902,\n 'team': 903,\n 'dr.': 904,\n 'though,': 905,\n 'spent': 906,\n 'novel': 907,\n 'tale': 908,\n 'show.': 909,\n 'japanese': 910,\n 'general': 911,\n 'scenes,': 912,\n 'monster': 913,\n 'tom': 914,\n 'meet': 915,\n 'third': 916,\n 'say,': 917,\n 'scene,': 918,\n 'unless': 919,\n 'meant': 920,\n 'deep': 921,\n 'attempts': 922,\n 'interested': 923,\n 'up,': 924,\n 'man,': 925,\n 'effort': 926,\n 'lady': 927,\n 'begin': 928,\n 'features': 929,\n 'pay': 930,\n 'worked': 931,\n 'now,': 932,\n 'soundtrack': 933,\n 'fast': 934,\n 'premise': 935,\n 'weak': 936,\n 'red': 937,\n 'scary': 938,\n 'reviews': 939,\n 'rock': 940,\n 'follow': 941,\n 'points': 942,\n '(a': 943,\n 'crap': 944,\n \"'the\": 945,\n 'sounds': 946,\n 'unfortunately,': 947,\n 'particular': 948,\n 'hardly': 949,\n 'space': 950,\n 'king': 951,\n 'expecting': 952,\n 'political': 953,\n 'viewers': 954,\n 'create': 955,\n 'appear': 956,\n 'future': 957,\n 'form': 958,\n 'parents': 959,\n 'her,': 960,\n 'oscar': 961,\n 'theater': 962,\n '20': 963,\n 'minute': 964,\n 'times,': 965,\n 'footage': 966,\n 'front': 967,\n 'gay': 968,\n 'storyline': 969,\n 'class': 970,\n 'inside': 971,\n 'older': 972,\n 'then,': 973,\n 'herself': 974,\n 'dog': 975,\n 'decides': 976,\n 'animation': 977,\n 'pure': 978,\n 'portrayed': 979,\n 'average': 980,\n 'visual': 981,\n 'sci-fi': 982,\n 'forward': 983,\n 'whatever': 984,\n 'fails': 985,\n 'slightly': 986,\n 'battle': 987,\n 'caught': 988,\n 'there.': 989,\n 'large': 990,\n 'result': 991,\n 'deserves': 992,\n \"we're\": 993,\n 'ask': 994,\n 'waiting': 995,\n 'release': 996,\n 'work.': 997,\n 'fantastic': 998,\n 'there,': 999,\n 'mystery': 1000,\n ...}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## use the dict to tokenize each review in reviews_split\n## store the tokenized reviews in reviews_ints\nreviews_ints = []\nfor review in reviews_split:\n    reviews_ints.append([vocab_to_int[word] for word in review.split()])\n    \nreviews_ints[:10]","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"[[8,\n  25,\n  241,\n  2,\n  163,\n  4,\n  1872,\n  17382,\n  189,\n  18,\n  1258,\n  3,\n  14,\n  2,\n  56855,\n  16,\n  189,\n  2420,\n  8,\n  6433,\n  23,\n  1,\n  114,\n  2647,\n  6,\n  80,\n  1,\n  125146,\n  203,\n  3418,\n  1,\n  5391,\n  4,\n  1,\n  3936,\n  7,\n  9,\n  19,\n  1,\n  43910,\n  499,\n  243,\n  5,\n  1026,\n  18,\n  6140,\n  13263,\n  11,\n  62,\n  1043,\n  3,\n  976,\n  5,\n  3116,\n  196,\n  16,\n  2,\n  105,\n  85736,\n  31386,\n  117,\n  26,\n  85737,\n  1,\n  5391,\n  4,\n  1,\n  125147,\n  12,\n  26282,\n  155,\n  27,\n  2595,\n  2638,\n  15,\n  67684,\n  115,\n  3,\n  2139,\n  106,\n  12,\n  125148,\n  67685,\n  2259,\n  62,\n  4,\n  39768,\n  3,\n  67685,\n  3647,\n  212,\n  4,\n  67686,\n  12,\n  6491,\n  21,\n  192,\n  23,\n  177,\n  82,\n  7,\n  3574,\n  37,\n  26,\n  4310,\n  112,\n  136,\n  214,\n  12423,\n  4,\n  411,\n  11,\n  474,\n  46,\n  26,\n  61,\n  2,\n  333,\n  4,\n  79,\n  3937,\n  5,\n  9786,\n  476,\n  45,\n  26,\n  6,\n  331,\n  1664,\n  41,\n  112,\n  67,\n  26,\n  13,\n  41,\n  1,\n  87,\n  655,\n  26,\n  6,\n  38,\n  14,\n  10652,\n  1,\n  415,\n  144,\n  6,\n  2,\n  105,\n  26283,\n  17,\n  11,\n  6,\n  80,\n  3759,\n  2125,\n  24975,\n  194,\n  5,\n  1032,\n  15,\n  303,\n  125149,\n  4,\n  1,\n  76,\n  3468,\n  576,\n  2953,\n  122,\n  10,\n  73,\n  89,\n  23,\n  336,\n  1,\n  2964,\n  18,\n  362,\n  5,\n  110,\n  6612,\n  86,\n  969,\n  15,\n  2,\n  7435,\n  22687,\n  3,\n  3452,\n  1155,\n  511,\n  4,\n  1,\n  233,\n  6311,\n  522,\n  73,\n  27,\n  7,\n  16,\n  2,\n  1137,\n  4,\n  2,\n  3512,\n  23794,\n  22,\n  5,\n  706,\n  10,\n  1,\n  85738,\n  25,\n  12158,\n  4393,\n  126,\n  1,\n  857,\n  4,\n  1,\n  1319,\n  83,\n  744,\n  46,\n  18,\n  1,\n  114,\n  4,\n  1,\n  31387,\n  13,\n  2791,\n  1589,\n  30,\n  9,\n  125,\n  11,\n  4700,\n  8958,\n  880,\n  4,\n  15423,\n  20839,\n  16337,\n  14590,\n  6371,\n  2041,\n  3,\n  36386,\n  11,\n  265,\n  1418,\n  41,\n  2,\n  1806,\n  7915,\n  34,\n  628,\n  940,\n  1514,\n  17,\n  11,\n  13,\n  41,\n  2,\n  155,\n  988,\n  62,\n  7,\n  8447,\n  242,\n  12158,\n  5,\n  151,\n  16338,\n  720,\n  20,\n  1971,\n  26,\n  581,\n  2,\n  5638,\n  34,\n  243,\n  15,\n  42,\n  211,\n  7195,\n  4433,\n  3,\n  8219,\n  66,\n  412,\n  5,\n  1574,\n  230,\n  79,\n  15,\n  2370,\n  3,\n  115,\n  17,\n  50,\n  6765,\n  2,\n  2056,\n  5,\n  772,\n  64,\n  61,\n  8707,\n  58,\n  161,\n  16,\n  1,\n  4701,\n  106,\n  12,\n  125150,\n  6,\n  31,\n  2623,\n  119,\n  30,\n  8448,\n  2723,\n  4,\n  2,\n  940,\n  3878,\n  262,\n  26,\n  40,\n  685,\n  5138,\n  26,\n  80,\n  40,\n  31,\n  2216,\n  7,\n  20,\n  588,\n  10,\n  40,\n  95,\n  100,\n  125151,\n  136,\n  192,\n  196,\n  100,\n  1324,\n  139,\n  5,\n  39769,\n  20,\n  1207,\n  6,\n  1777,\n  20,\n  9069,\n  1187,\n  7,\n  232,\n  11,\n  408,\n  31,\n  3469,\n  16,\n  2203,\n  12,\n  39770,\n  6,\n  1,\n  23795,\n  670,\n  10,\n  701,\n  2,\n  326,\n  5,\n  23796,\n  55,\n  581,\n  1741,\n  2352,\n  136,\n  101,\n  7,\n  8219,\n  6969,\n  55,\n  80,\n  581,\n  2,\n  6613,\n  487,\n  10,\n  8103,\n  760,\n  55,\n  2241,\n  5,\n  1801,\n  100,\n  126,\n  2,\n  1100,\n  4,\n  1571,\n  12,\n  6491,\n  102,\n  151,\n  115,\n  15,\n  29,\n  3033,\n  22,\n  1,\n  430,\n  85739,\n  450,\n  22688,\n  229,\n  4,\n  8329,\n  17,\n  2,\n  115,\n  104,\n  2370,\n  3,\n  1782,\n  831,\n  1300,\n  16,\n  102,\n  88,\n  34,\n  25,\n  70,\n  8330,\n  32,\n  151,\n  1801,\n  39771,\n  24,\n  6,\n  22689,\n  3,\n  4205,\n  1,\n  8331,\n  11,\n  6,\n  228,\n  134,\n  1039,\n  5,\n  1,\n  29403,\n  2063,\n  10,\n  11,\n  444,\n  5,\n  125152,\n  10,\n  416,\n  2468,\n  4836,\n  42,\n  443,\n  63,\n  22,\n  4268,\n  3,\n  68,\n  65,\n  142,\n  42,\n  24,\n  18555,\n  6551,\n  1512,\n  78,\n  9,\n  24,\n  1262,\n  7,\n  1,\n  1021,\n  1378,\n  59,\n  29,\n  65,\n  9,\n  24,\n  18,\n  263,\n  1469,\n  56,\n  2222,\n  3,\n  1735,\n  66,\n  952,\n  16855,\n  3,\n  1,\n  128,\n  1434,\n  6,\n  50,\n  36,\n  1174,\n  10091,\n  22,\n  257,\n  2738,\n  125153,\n  125154,\n  3830,\n  7060,\n  6,\n  1277,\n  2,\n  323,\n  1807,\n  17,\n  207,\n  458,\n  234,\n  2,\n  24,\n  10,\n  55,\n  194,\n  5,\n  27,\n  31,\n  43911,\n  863,\n  1,\n  1075,\n  125155,\n  4,\n  14219,\n  863,\n  1,\n  1075,\n  125156,\n  4,\n  136,\n  23,\n  66,\n  3292,\n  69,\n  9,\n  13264,\n  67687,\n  12,\n  733,\n  55,\n  6,\n  5259,\n  2,\n  125157,\n  258,\n  10,\n  60,\n  537,\n  107,\n  18,\n  1981,\n  18,\n  2,\n  848,\n  10,\n  6,\n  48,\n  567,\n  5,\n  1,\n  22690,\n  1,\n  1496,\n  2624,\n  83,\n  272,\n  8959,\n  28,\n  35,\n  3,\n  1,\n  206,\n  3831,\n  1562,\n  340,\n  30,\n  2,\n  496,\n  656,\n  1216,\n  145,\n  16,\n  169,\n  31,\n  125158,\n  333,\n  4,\n  16339,\n  12,\n  1972,\n  185,\n  99,\n  2648,\n  166,\n  125159,\n  627,\n  7,\n  232,\n  156,\n  332,\n  169,\n  5350,\n  38,\n  556,\n  9,\n  125160,\n  24,\n  6,\n  2,\n  48,\n  54,\n  8827,\n  109,\n  78,\n  1,\n  1538,\n  979,\n  58,\n  870,\n  13,\n  43912,\n  77,\n  6,\n  125161,\n  597,\n  186,\n  41,\n  10653,\n  34,\n  6,\n  7,\n  115,\n  15,\n  125162,\n  71,\n  125163,\n  378,\n  408,\n  125164,\n  3648,\n  58,\n  6,\n  2,\n  125165,\n  32,\n  4071,\n  71,\n  1,\n  125166,\n  378,\n  832,\n  125167,\n  56856,\n  67688,\n  378,\n  408,\n  3309,\n  17,\n  55,\n  6,\n  22,\n  1196,\n  5,\n  56857,\n  19,\n  6,\n  48,\n  20057,\n  5260,\n  21,\n  125168,\n  871,\n  21,\n  36387,\n  5,\n  125169,\n  85740,\n  21,\n  20057,\n  2708,\n  690,\n  6,\n  48,\n  125170,\n  66,\n  2569,\n  13265,\n  813,\n  6,\n  67689,\n  24,\n  6,\n  1308,\n  14591,\n  7,\n  125171,\n  216,\n  145,\n  1,\n  19,\n  86,\n  5,\n  125172,\n  1,\n  350,\n  3,\n  49362,\n  125173,\n  1047,\n  1,\n  7811,\n  4,\n  102,\n  497,\n  3,\n  102,\n  14592,\n  7,\n  2,\n  2849,\n  14220,\n  15,\n  1,\n  886,\n  1531,\n  5639,\n  18,\n  1,\n  4903,\n  125174,\n  11,\n  212,\n  1,\n  11465,\n  16,\n  1,\n  2806,\n  15,\n  67690,\n  13891,\n  5,\n  1,\n  413,\n  4,\n  58,\n  4864,\n  1831,\n  11914,\n  17956,\n  4,\n  6829,\n  125175,\n  6,\n  1791,\n  3,\n  20,\n  1269,\n  3879,\n  125176,\n  33659,\n  4,\n  67691,\n  125177,\n  36388,\n  136,\n  27725,\n  1150,\n  3022,\n  2003,\n  49363,\n  2639,\n  85741,\n  4,\n  125178,\n  3,\n  20,\n  125179,\n  539,\n  125180,\n  22691,\n  125181,\n  4,\n  219,\n  2326,\n  67692,\n  19288,\n  1,\n  125182,\n  9,\n  17957,\n  4,\n  43913,\n  116,\n  21,\n  554,\n  7,\n  2,\n  12682,\n  4311,\n  2,\n  56858,\n  125183,\n  85742,\n  176,\n  125184,\n  2697,\n  15858,\n  3,\n  622,\n  15858,\n  341,\n  186,\n  360,\n  165,\n  125185,\n  11247,\n  4364,\n  4865,\n  1,\n  490,\n  4,\n  7061,\n  43914,\n  134,\n  28,\n  221,\n  3601,\n  271,\n  10,\n  52,\n  25,\n  17383,\n  212,\n  9,\n  3427,\n  3719,\n  41,\n  78,\n  2669,\n  18556,\n  7,\n  2,\n  4837,\n  3,\n  377,\n  58,\n  211,\n  238,\n  43915,\n  12,\n  247,\n  7355,\n  5139,\n  343,\n  23,\n  1,\n  125186,\n  3,\n  93,\n  23,\n  14221,\n  3,\n  4565,\n  76,\n  1538,\n  1138,\n  196,\n  1661,\n  133,\n  484,\n  8220,\n  13891,\n  5,\n  58,\n  4434,\n  3,\n  2296,\n  30,\n  2,\n  2143,\n  4646,\n  10092,\n  13566,\n  9,\n  49364,\n  3980,\n  2,\n  1950,\n  1951,\n  1360,\n  10848,\n  36389,\n  4,\n  67693,\n  125187,\n  20,\n  1273,\n  105,\n  728,\n  10654,\n  125188,\n  125189,\n  4,\n  20840,\n  4,\n  1,\n  125190,\n  17384,\n  1,\n  11465,\n  15,\n  58,\n  85743,\n  1831,\n  67694,\n  196,\n  1360,\n  49,\n  26,\n  444,\n  5,\n  67695,\n  819,\n  17,\n  7,\n  1,\n  8828,\n  1,\n  3760,\n  6766,\n  7,\n  58,\n  14220,\n  85744,\n  3,\n  32,\n  2478,\n  62,\n  18,\n  22692,\n  49365,\n  32,\n  7812,\n  2,\n  1818,\n  15,\n  1360,\n  96,\n  32,\n  11248,\n  10654,\n  62,\n  7,\n  1,\n  9625,\n  4,\n  1,\n  85743,\n  55,\n  2941,\n  2,\n  3229,\n  126,\n  42,\n  4434,\n  3,\n  2296,\n  3,\n  11,\n  6,\n  125191,\n  15,\n  4904,\n  ...]]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(reviews_ints)","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stats about vocabulary\nprint('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\nprint()\n\n# print tokens in first review\n#print('Tokenized review: \\n', reviews_ints[:1])","execution_count":12,"outputs":[{"output_type":"stream","text":"Unique words:  334972\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1=positive, 0=negative label conversion\n#labels_split = labels.split('')\n#encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier review stats\nreview_lens = Counter([len(x) for x in reviews_ints])\nprint(\"Zero-length reviews: {}\".format(review_lens[0]))\nprint(\"Maximum review length: {}\".format(max(review_lens)))","execution_count":18,"outputs":[{"output_type":"stream","text":"Zero-length reviews: 0\nMaximum review length: 8072009\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels])","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of reviews before removing outliers: ', len(reviews_ints))\n\n## remove any reviews/labels with zero length from the reviews_ints list.\n\n# get indices of any reviews with length 0\nnon_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n\n# remove 0-length reviews and their labels\nreviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\nencoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n\nprint('Number of reviews after removing outliers: ', len(reviews_ints))","execution_count":22,"outputs":[{"output_type":"stream","text":"Number of reviews before removing outliers:  1\nNumber of reviews after removing outliers:  1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test your implementation!\n\nseq_length = 200\n\nfeatures = pad_features(reviews_ints, seq_length=seq_length)\n\n## test statements - do not change - ##\nassert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\nassert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n\n# print first 10 values of the first 30 batches \nprint(features[:30,:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"split_frac = 0.8\n\n## split data into training, validation, and test data (features and labels, x and y)\n\nsplit_idx = int(len(features)*split_frac)\ntrain_x, remaining_x = features[:split_idx], features[split_idx:]\ntrain_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\n## print out the shapes of your resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# dataloaders\nbatch_size = 50\n\n# make sure the SHUFFLE your training data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\n\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        x = x.long()\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_size, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training params\n\nepochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output, h = net(inputs, h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# negative test review\ntest_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\n\ndef tokenize_review(test_review):\n    test_review = test_review.lower() # lowercase\n    # get rid of punctuation\n    test_text = ''.join([c for c in test_review if c not in punctuation])\n\n    # splitting by spaces\n    test_words = test_text.split()\n\n    # tokens\n    test_ints = []\n    test_ints.append([vocab_to_int[word] for word in test_words])\n\n    return test_ints\n\n# test code and generate tokenized review\ntest_ints = tokenize_review(test_review_neg)\nprint(test_ints)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test sequence padding\nseq_length=200\nfeatures = pad_features(test_ints, seq_length)\n\nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test conversion to tensor and pass into your model\nfeature_tensor = torch.from_numpy(features)\nprint(feature_tensor.size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(net, test_review, sequence_length=200):\n    \n    net.eval()\n    \n    # tokenize review\n    test_ints = tokenize_review(test_review)\n    \n    # pad tokenized sequence\n    seq_length=sequence_length\n    features = pad_features(test_ints, seq_length)\n    \n    # convert to tensor to pass into your model\n    feature_tensor = torch.from_numpy(features)\n    \n    batch_size = feature_tensor.size(0)\n    \n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n    \n    if(train_on_gpu):\n        feature_tensor = feature_tensor.cuda()\n    \n    # get the output from the model\n    output, h = net(feature_tensor, h)\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze()) \n    # printing output value, before rounding\n    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n    \n    # print custom response\n    if(pred.item()==1):\n        print(\"Positive review detected!\")\n    else:\n        print(\"Negative review detected.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# positive test review\ntest_review_pos = 'This movie had the best acting and the dialogue was so good. I loved it.'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# call function\nseq_length=200 # good to use the length that was trained on\n\npredict(net, test_review_neg, seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['review'].str.lower().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews = []\nfor r in data['review'].str.lower():\n    reviews.append(r)\nreviews[5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text = ''.join([c for c in reviews if c not in punctuation])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_split = all_text.split('\\n')\nall_text = ' '.join(reviews_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_text[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words = all_text.split()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"words[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#data['review'] = data['review'].str.lower()\n#all_text = ' '.join([c for c in data['review'] if c not in punctuation])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data['review'].head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Build a dictionary that maps words to integers\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\n## use the dict to tokenize each review in reviews_split\n## store the tokenized reviews in reviews_ints\nreviews_ints = []\nfor review in reviews_split:\n    reviews_ints.append([vocab_to_int[word] for word in review.split()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reviews_ints.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stats about vocabulary\nprint('Unique words: ', len((vocab_to_int)))  # should ~ 74000+\nprint()\n\n# print tokens in first review\n#print('Tokenized review: \\n', reviews_ints[:1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# outlier review stats\nreview_lens = Counter([len(x) for x in reviews_ints])\nprint(\"Zero-length reviews: {}\".format(review_lens[0]))\nprint(\"Maximum review length: {}\".format(max(review_lens)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pad_features(reviews_ints, seq_length):\n    ''' Return features of review_ints, where each review is padded with 0's \n        or truncated to the input seq_length.\n    '''\n    \n    # getting the correct rows x cols shape\n    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n\n    # for each review, I grab that review and \n    for i, row in enumerate(reviews_ints):\n        features[i, -len(row):] = np.array(row)[:seq_length]\n    \n    return features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test your implementation!\n\nseq_length = 100\n\nfeatures = pad_features(reviews_ints, seq_length=seq_length)\n\n## test statements - do not change - ##\nassert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\nassert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n\n# print first 10 values of the first 30 batches \nprint(features[:30,:10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}